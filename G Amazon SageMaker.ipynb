{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 310](https://github.com/GonzagaCPSC310) Data Mining\n",
    "[Gonzaga University](https://www.gonzaga.edu/)\n",
    "\n",
    "[Gina Sprint](http://cs.gonzaga.edu/faculty/sprint/)\n",
    "\n",
    "# Amazon SageMaker\n",
    "What are our learning objectives for this lesson?\n",
    "* Make an Amazon Web Services (AWS) account\n",
    "* Set up a Jupyter Notebook instance on Amazon SageMaker\n",
    "* Perform simple k-means clustering using SageMaker\n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* [Amazon SageMaker developer guide](https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up Task(s)\n",
    "1. Make a [free AWS account](https://aws.amazon.com/free/?awsf.Free%20Tier%20Types=categories%23featured) if you don't already have one\n",
    "    * Later, you can get free AWS credits as a student through AWS Educate: https://aws.amazon.com/education/awseducate/ \n",
    "1. Go to console.aws.amazon.com and sign in to your account\n",
    "1. Make sure you have ClusteringFun/shirt_sizes.csv downloaded on your machine somewhere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful Links\n",
    "* [Amazon SageMaker developer guide](https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html)\n",
    "* Instance types:\n",
    "    * [Amazon E2 instance types](https://aws.amazon.com/ec2/instance-types/)\n",
    "    * [Amazon SageMaker ML instance types](https://aws.amazon.com/sagemaker/pricing/instance-types/)\n",
    "* [SageMaker pricing](https://aws.amazon.com/sagemaker/pricing/)\n",
    "* [S3 pricing](https://aws.amazon.com/s3/pricing/)\n",
    "* [SageMaker KMeans documentation](https://sagemaker.readthedocs.io/en/stable/kmeans.html)\n",
    "* [JupyterLab vs Jupyter Notebook](https://towardsdatascience.com/jupyter-lab-evolution-of-the-jupyter-notebook-5297cacde6b)\n",
    "\n",
    "## Setup\n",
    "1. S3 Console:\n",
    "    1. Create a S3 bucket\n",
    "    1. Upload your dataset (in this example, shirt_sizes.csv) to the bucket\n",
    "1. SageMaker Console:\n",
    "    1. Create a new notebook instance\n",
    "        1. Create a new IAM role\n",
    "    1. Make sure your instance region is the same as your S3 bucket region\n",
    "    1. Wait for it to initialize, then open it in Jupyter\n",
    "1. Jupyter Dashboard:\n",
    "    1. Create a new Jupyter notebook with conda-python3 kernel\n",
    "    1. Rename the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "### Using SageMaker from Python\n",
    "Sagemaker APIs can be accessed one of two ways from Python:\n",
    "1. The low-level Python SDK... this is the `boto3` related code you might see in the tutorials\n",
    "1. The high level Python API... this is the `sagemaker` related code you migth see in the tutorials\n",
    "    1. Alot of things are customizable with option 1 that are abstracted with option 2\n",
    "    1. Probably start with option 2 until you need option 1 to keep things simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role() # get the IAM role you associated with this notebook when you created it\n",
    "# essentially authentication... for use when accessing S3 buckets, endpoints, etc.\n",
    "bucket = \"sagemaker-temp2\"\n",
    "original_data_key = \"shirt_sizes.csv\"\n",
    "original_data_location = \"s3://\" + bucket + \"/\" + original_data_key\n",
    "print(\"original data path:\", original_data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data for Amazon's K-Means Algorithm\n",
    "Next we need to read the data from S3, clean it, and convert it to a numpy ndarray which is one type the ML algorithms can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(original_data_location)\n",
    "df.columns = df.columns.str.strip()\n",
    "df = df.drop(\"size(t-shirt)\", axis=1)\n",
    "# normalize the data\n",
    "df = (df - df.min()) / (df.max() - df.min())\n",
    "print(df)\n",
    "# convert the data values to be numpy float32s\n",
    "df = df.astype(np.float32)\n",
    "train_set = df.values\n",
    "print(train_set)\n",
    "print(type(train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up K-Means\n",
    "We can use any of the following algorithms:\n",
    "1. Ones Amazon provides via `sagemaker`\n",
    "1. Ones in the Amazon Marketplace\n",
    "1. Ones we write ourselves\n",
    "    1. Write in Notebook\n",
    "    1. Write offline and upload/convert\n",
    "\n",
    "We will use `KMeans` from `sagemaker`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import KMeans\n",
    "\n",
    "# sagemaker will convert our numpy array to a byte format called recordIO that it is more optimal to work with\n",
    "# we need to say where to store that data\n",
    "data_key = original_data_key.split(\".\")[0] + \"_recordIO_data\"\n",
    "data_location = \"s3://\" + bucket + \"/\" + data_key\n",
    "output_key = original_data_key.split(\".\")[0] + \"_output\"\n",
    "output_location = \"s3://\" + bucket + \"/\" + output_key\n",
    "\n",
    "print(\"training data will be uploaded to\", data_location)\n",
    "print(\"training artifacts will be uploaded to\", output_location)\n",
    "\n",
    "kmeans = KMeans(role=role,\n",
    "               train_instance_count=1,\n",
    "               train_instance_type=\"ml.m4.xlarge\",\n",
    "               output_path=output_location,\n",
    "               k=2,\n",
    "               data_location=data_location)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# magic command, times the duration to execute the cell\n",
    "\n",
    "kmeans.fit(kmeans.record_set(train_set)) # runs asynchronously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Model\n",
    "So the model has been written out to a model.tar.gz file in S3... kind of a pain to inspect if you ask me!\n",
    "\n",
    "Anways, we need to \"deploy\" the model to an endpoint in order to start running instances through it for prediction/clustering/etc. Predictions are returned via JSON object (note: you can also make an HTTP request for a prediction of your endpoint and get a JSON response back as well... this is neat because you can use your model in your apps!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "kmeans_predictor = kmeans.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen = np.array([[0.5, 0.5]], dtype=np.float32)\n",
    "result = kmeans_predictor.predict(unseen)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips on Viewing the Algorithm Parameters\n",
    "This would be if you wanted to \"view your model.\" For k-means, this would be seeing the cluster centroids. This takes more work than I think it should because the `sagemaker.KMeans` class does not expose the centroids, it instead writes them to the model file, model.tar.gz\n",
    "\n",
    "* You'll need to create a training job using the low-level SDK for Python so you have the job_name in your code. See this part of the developer guide for how to do this: https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-train-model-create-training-job.html\n",
    "    * I'll note that you could manually go to your S3 bucket and find the job name that was automatically used via `kmeans.fit(kmeans.record_set(train_set[0]))` of the high-level Python library. It will be the name of the folder storing a model.tar.gz file, something like `kmeans-2019-04-29-02-11-06-721`\n",
    "* You'll need to load the trained model via https://aws.amazon.com/blogs/machine-learning/analyze-us-census-data-for-population-segmentation-using-amazon-sagemaker/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
